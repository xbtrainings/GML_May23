{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 : Graph Isomorphism Networks - exercise\n",
    "\n",
    "Xu, Hu, Leskovec, Jegelka, How powerful are graph neural networks, 2018   \n",
    "https://arxiv.org/pdf/1810.00826.pdf\n",
    "\n",
    "### Xavier Bresson  \n",
    "\n",
    "<br>\n",
    "Notebook goals :<br>  \n",
    "• Implement GIN with DGL <br> \n",
    "• Compare performance with node in-degree feature and positional encoding as Laplacian eigenvectors <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/GML_May23_codes/codes/06_WL_GNN'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl # Install DGL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import dgl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from utils import compute_ncut\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate CSL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CSL_dataset(num_nodes, list_skip_value, num_graph_per_class):\n",
    "    label_class = 0\n",
    "    list_graph = []\n",
    "    # loop over skip values\n",
    "    for skip_value in list_skip_value:\n",
    "        # build CSL graph with node indexing in {0,1,...,num_nodes-1}\n",
    "        list_src = []\n",
    "        list_dst = []\n",
    "        for i in range(num_nodes):\n",
    "            # cycle\n",
    "            list_src.append(i)\n",
    "            list_dst.append((i+1)%num_nodes)\n",
    "            # skip connection\n",
    "            list_src.append(i)\n",
    "            list_dst.append((i+skip_value)%num_nodes)\n",
    "        graph = dgl.graph((list_src, list_dst)) # Build DGL graph\n",
    "        graph = dgl.to_bidirected(graph) # symmetrize/undirected graph\n",
    "        list_graph.append([graph, torch.tensor(label_class).long()])\n",
    "        for _ in range(num_graph_per_class-1): # build CSL graph with random node indexing\n",
    "            idx_shuffle = torch.randperm(num_nodes).tolist() # random permutation of node indexing\n",
    "            list_src = [idx_shuffle[i] for i in list_src]\n",
    "            list_dst = [idx_shuffle[i] for i in list_dst]\n",
    "            graph = dgl.graph((list_src, list_dst)) # Build DGL graph\n",
    "            graph = dgl.to_bidirected(graph) # symmetrize/undirected graph\n",
    "            list_graph.append([graph, torch.tensor(label_class).long()])\n",
    "        # increment class label\n",
    "        label_class += 1\n",
    "    return list_graph\n",
    "\n",
    "\n",
    "# Generate small CSL graph (for understanding)\n",
    "num_nodes = 7; list_skip_value = [2]; num_graph_per_class = 1\n",
    "small_csl_graph = generate_CSL_dataset(num_nodes, list_skip_value, num_graph_per_class)\n",
    "print(small_csl_graph[0])\n",
    "graph = small_csl_graph[0][0]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "nx.draw(graph.to_networkx(), ax=ax, with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate train and test datasets \n",
    "\n",
    "## Add node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplacian eigenvectors as positional encoding\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node and edge features to graphs\n",
    "pos_enc_dim = 2 # dimension of PE\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['in_degree'] = graph.in_degrees().view(-1, 1).float() # node feature is node in-degree\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding feature \n",
    "    return dataset\n",
    "\n",
    "# Generate CSL datasets\n",
    "num_nodes = 41\n",
    "list_skip_value = [2, 3, 4, 5, 6, 9, 11, 12, 13, 16]\n",
    "num_graph_per_class = 15\n",
    "trainset = generate_CSL_dataset(num_nodes, list_skip_value, num_graph_per_class)\n",
    "testset = generate_CSL_dataset(num_nodes, list_skip_value, num_graph_per_class)\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embedding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the collate function to prepare a batch of DGL graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.tensor(labels) # batch of labels (here class label)\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['in_degree']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the class of Graph Isomorphism Networks with DGL\n",
    "\n",
    "Node update equation:   \n",
    "\\begin{eqnarray}\n",
    "h_i^{\\ell+1} &=& \\textrm{MLP} \\left( (1+\\varepsilon^{\\ell})\\ h_i^{\\ell} + \\sum_{j\\sim i} h_j^{\\ell} \\right)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP layer for classification\n",
    "class MLP_layer(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, L=2): # L = nb of hidden layers\n",
    "        super(MLP_layer, self).__init__()\n",
    "        list_FC_layers = [ nn.Linear( input_dim, input_dim, bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(nn.Linear( input_dim, output_dim , bias=True ))\n",
    "        self.FC_layers = nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            y = torch.relu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y\n",
    "\n",
    "        \n",
    "# class of GatedGCN layer  \n",
    "class GIN_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GIN_layer, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.linear2 = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        # Define a learnable scalar parameter initialized to 0.001\n",
    "        # You may use \"nn.Parameter()\"\n",
    "        self.eps = ### YOUR CODE HERE, scalar, size=()\n",
    "        \n",
    "    # Step 1 of message-passing with DGL: \n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i) \n",
    "    def message_func(self, edges):\n",
    "        hj = edges.src['h'] \n",
    "        return {'hj' : hj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL: \n",
    "    #   Reduce function collects all messages={hj} sent to node dst/i with Step 1\n",
    "    def reduce_func(self, nodes):\n",
    "        hj = nodes.mailbox['hj'] # size=(V,|Nj|,d), |Nj|=num_neighbors\n",
    "        # Compute hi = sum_j hj\n",
    "        sum_hj = ### YOUR CODE HERE, size=(V,d)\n",
    "        return {'sum_hj' : sum_hj} \n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        g.ndata['h'] = h \n",
    "        g.update_all(self.message_func,self.reduce_func) # update the node feature with DGL\n",
    "        # Compute GIN node update \n",
    "        # You may use \"nn.Linear()\", \"torch.relu()\"\n",
    "        h = ### YOUR CODE HERE, size=(V,d)\n",
    "        return h\n",
    "    \n",
    "    \n",
    "class GIN_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GIN_net, self).__init__()\n",
    "        in_degree_dim = net_parameters['in_degree_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        output_dim = net_parameters['output_dim']\n",
    "        L = net_parameters['L']\n",
    "        #self.embedding_h = nn.Linear(in_degree_dim, hidden_dim) # node in-degree\n",
    "        self.embedding_h = nn.Linear(pos_enc_dim, hidden_dim) # node PE with Lap eigenvectors\n",
    "        self.GIN_layers = nn.ModuleList([ GIN_layer(hidden_dim, hidden_dim) for _ in range(L) ]) \n",
    "        self.MLP_layer = MLP_layer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        \n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)\n",
    "        \n",
    "        # GIN layers\n",
    "        for GINlayer in self.GIN_layers:\n",
    "            h = GINlayer(g,h)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        y = dgl.sum_nodes(g,'h')\n",
    "        y = self.MLP_layer(y)\n",
    "        \n",
    "        return y    \n",
    "    \n",
    "    def loss(self, y_scores, y_labels):\n",
    "        loss = nn.CrossEntropyLoss()(y_scores, y_labels)\n",
    "        return loss        \n",
    "        \n",
    "    def accuracy(self, scores, targets):\n",
    "        scores = scores.detach().argmax(dim=1)\n",
    "        acc = (scores==targets).float().sum().item()\n",
    "        return acc\n",
    "\n",
    "\n",
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['in_degree_dim'] = 1\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['output_dim'] = len(list_skip_value) # nb of classes\n",
    "net_parameters['L'] = 4\n",
    "net = GIN_net(net_parameters)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_one_epoch(net, data_loader, train=True):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_data = 0\n",
    "    gpu_mem = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['in_degree']\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_labels = batch_labels\n",
    "        #batch_scores = net.forward(batch_graphs, batch_x) # node in-degree\n",
    "        batch_scores = net.forward(batch_graphs, batch_pe) # node PE with Lap eigenvectors\n",
    "        loss = net.loss(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_acc += net.accuracy(batch_scores,batch_labels)\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_acc /= nb_data\n",
    "    return epoch_loss, epoch_acc \n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "train_loader = DataLoader(trainset, batch_size=10, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=10, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['in_degree_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 256\n",
    "net_parameters['output_dim'] = len(list_skip_value) # nb of classes\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GIN_net(net_parameters)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(201):\n",
    "    epoch_train_loss, epoch_train_acc = run_one_epoch(net, train_loader, True)\n",
    "    if not epoch%10:\n",
    "        with torch.no_grad(): \n",
    "            epoch_test_loss, epoch_test_acc = run_one_epoch(net, test_loader, False)\n",
    "        print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "        print('                      train_acc: {:.4f}, test_acc: {:.4f}'.format(epoch_train_acc, epoch_test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
